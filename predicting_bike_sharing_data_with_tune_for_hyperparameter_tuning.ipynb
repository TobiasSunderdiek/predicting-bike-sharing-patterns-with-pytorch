{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "predicting_bike_sharing_data_with_tune_for_hyperparameter_tuning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TobiasSunderdiek/predicting-bike-sharing-patterns-with-pytorch/blob/master/predicting_bike_sharing_data_with_tune_for_hyperparameter_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z0eKFKr8neG",
        "colab_type": "text"
      },
      "source": [
        "# Predicting bike sharing data with tune for hyperparameter tuning\n",
        "\n",
        "This notebook does a regression task on the bikesharing dataset, which I know from the udacity project [1]. I build a pytorch model similar to the numpy/pandas-based version in the udacity project and tune the hyperparameters with tune [2].\n",
        "\n",
        "[1] https://github.com/udacity/deep-learning-v2-pytorch/blob/master/project-bikesharing/Predicting_bike_sharing_data.ipynb\n",
        "\n",
        "[2] https://ray.readthedocs.io/en/latest/tune.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85c1-HDvsrmn",
        "colab_type": "text"
      },
      "source": [
        "## Prepare for using tune\n",
        "For use in colab, pyarrow has to be uninstalled. After that, colab runtime has be be restarted.\n",
        "\n",
        "Additionally, clean-up log dir for tune logs and install hyperopt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lci88lLVTmd5",
        "colab_type": "code",
        "outputId": "e116cf03-c708-411e-add9-73e63d188fd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        }
      },
      "source": [
        "# preparing for using tune\n",
        "!pip install ray\n",
        "!pip uninstall -y pyarrow\n",
        "# cleanup tune log dir if exists\n",
        "!rm -rf tune_logs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ray\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/83/59ddfbd982626f5fa58f0ab9ea201757b1ccc34ba3b3bdf44a61a3a7d8b1/ray-0.7.6-cp36-cp36m-manylinux1_x86_64.whl (75.8MB)\n",
            "\u001b[K     |████████████████████████████████| 75.8MB 105kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from ray) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.6/dist-packages (from ray) (1.17.4)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from ray) (3.6.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from ray) (7.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from ray) (3.10.0)\n",
            "Collecting redis>=3.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/ae/28613a62eea0d53d3db3147f8715f90da07667e99baeedf1010eb400f8c0/redis-3.3.11-py2.py3-none-any.whl (66kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 579kB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from ray) (3.13)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from ray) (2.6.0)\n",
            "Collecting funcsigs\n",
            "  Downloading https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from ray) (3.0.12)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/4f/a6/728666f39bfff1719fc94c481890b2106837da9318031f71a8424b662e12/colorama-0.4.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->ray) (7.2.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->ray) (1.8.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->ray) (1.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->ray) (41.4.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->ray) (0.7.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->ray) (19.3.0)\n",
            "Installing collected packages: redis, funcsigs, colorama, ray\n",
            "Successfully installed colorama-0.4.1 funcsigs-1.0.2 ray-0.7.6 redis-3.3.11\n",
            "Uninstalling pyarrow-0.14.1:\n",
            "  Successfully uninstalled pyarrow-0.14.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9O7_R1ZyTKuD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip uninstall -y albumentations\n",
        "!pip install --upgrade git+git://github.com/hyperopt/hyperopt.git\n",
        "!pip install setproctitle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jb5V5Sb09V4W",
        "colab_type": "text"
      },
      "source": [
        "## Loading dataset from github\n",
        "The original dataset is located here:\n",
        "\n",
        "https://raw.githubusercontent.com/udacity/deep-learning-v2-pytorch/master/project-bikesharing/Bike-Sharing-Dataset/hour.csv\n",
        "\n",
        "which originaly came from [1].\n",
        "\n",
        "[1] Fanaee-T, Hadi, and Gama, Joao, \"Event labeling combining ensemble detectors and background knowledge\", Progress in Artificial Intelligence (2013): pp. 1-15, Springer Berlin Heidelberg, doi:10.1007/s13748-013-0040-3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cT4j5Qg2o9Yg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fetch a single file using the raw GitHub URL.\n",
        "!curl --remote-name \\\n",
        "     -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "     --location https://raw.githubusercontent.com/udacity/deep-learning-v2-pytorch/master/project-bikesharing/Bike-Sharing-Dataset/hour.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVMh2OG7rGpx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "rides = pd.read_csv(\"/content/hour.csv\")\n",
        "rides_origin = rides\n",
        "rides.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "segnb5Shu-1S",
        "colab_type": "text"
      },
      "source": [
        "### Convert data\n",
        "I do nearly the same converting as in the udacity project mentioned in [1].\n",
        "- hot-encode categorical features *season*, *weathersit*, *mnth*, *hr*, *weekday* and drop origin of this features\n",
        "- drop fields *instant*, *dteday*, *atemp* and *workingday* as in the udacity project\n",
        "- additionally drop fields *casual* and *registered*, focus on overall output *cnt*\n",
        "- shift and scale continuous features *cnt*, *temp*, *hum*, *windspeed* so they have zero mean and standard deviation of 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rqqljiastz8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for feature in ['season', 'weathersit', 'mnth', 'hr', 'weekday']:\n",
        "  hot_encoded_features = pd.get_dummies(rides[feature], prefix=feature, drop_first=False)\n",
        "  rides = pd.concat([rides, hot_encoded_features], axis=1)\n",
        "  rides = rides.drop(feature, axis=1)\n",
        "rides = rides.drop(['instant', 'dteday', 'atemp', 'workingday', 'casual', 'registered'], axis=1)\n",
        "\n",
        "feature_scaling_store = {}\n",
        "\n",
        "for feature in ['cnt', 'temp', 'hum', 'windspeed']:\n",
        "  mean, std = rides[feature].mean(), rides[feature].std()\n",
        "  feature_scaling_store[feature] = [mean, std]\n",
        "  rides.loc[:, feature] = (rides[feature] - mean) / std\n",
        "\n",
        "rides.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFXhUrU9895a",
        "colab_type": "text"
      },
      "source": [
        "### Split into training,  testing and validation set\n",
        "I do the same converting as in the udacity project mentioned in [1].\n",
        "\n",
        "The data consists of entries of how many bikes are rented at one specific hour of the day. The total number of entries in the hour.csv is 17.379, which means divided by 24 there are datapoints for approximatly 724 days.\n",
        "\n",
        "The last 21 days (3%) are used as testing data.\n",
        "\n",
        "Of the remaining days, 60 days (8.5%) are used as validation data.\n",
        "\n",
        "The training data consists of 643 days."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckJMWC_GmfUM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = rides[-21*24:]\n",
        "rides = rides[:-21*24]\n",
        "\n",
        "validation_data = rides[-60*24:]\n",
        "rides = rides[:-60*24]\n",
        "\n",
        "train_data = rides\n",
        "\n",
        "target_fields = ['cnt']\n",
        "\n",
        "features_train, targets_train = train_data.drop(target_fields, axis=1), train_data[target_fields]\n",
        "features_validation, targets_validation = validation_data.drop(target_fields, axis=1), validation_data[target_fields]\n",
        "features_test, targets_test = test_data.drop(target_fields, axis=1), test_data[target_fields]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFzFb1t_BqB1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "train_set = TensorDataset(torch.tensor(features_train.values).float(),\n",
        "                          torch.tensor(targets_train.values).float())\n",
        "\n",
        "valid_set = TensorDataset(torch.tensor(features_validation.values).float(),\n",
        "                          torch.tensor(targets_validation.values).float())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiIjayNCsbK3",
        "colab_type": "text"
      },
      "source": [
        "## Network architecture\n",
        "I define the necessary parts of the architecture and try them out before handing over to tune.\n",
        "\n",
        "I observed, that there is a difference in using float or double and bias/no bias in the network. At least, the losses oscilatte much more with floats than with doubles. I compared the original numpy/panda-based version of the network from udacity with a pytorch version of my own in https://github.com/TobiasSunderdiek/predicting-bike-sharing-patterns-with-pytorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQqLWSXY44AG",
        "colab_type": "text"
      },
      "source": [
        "### Hyperparameter\n",
        "\n",
        "defined as map in `hyperparameter`, e.g.:\n",
        "\n",
        " `hyperparameter = {'learning_rate': 0.01, 'hidden_nodes': 25, 'epochs': 4000, 'batch_size': 1}`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9EJaIYW2czS",
        "colab_type": "text"
      },
      "source": [
        "### Model\n",
        "\n",
        "Size of input node is equal to columns in feature data\n",
        "```\n",
        "input_nodes = features_train.shape[1]\n",
        "\n",
        "bikeSharingModel = BikeSharingModel(input_nodes, hyperparameter['hidden_nodes'])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PT-x4JQl6eM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "class BikeSharingModel(nn.Module):\n",
        "  def __init__(self, input_nodes, hidden_nodes):\n",
        "    super(BikeSharingModel, self).__init__()\n",
        "    self.fc_1 = nn.Linear(input_nodes, hidden_nodes, bias=False)\n",
        "    self.fc_2 = nn.Linear(hidden_nodes, 1, bias=False)\n",
        " \n",
        "  def forward(self, x):\n",
        "    x = self.fc_1(x)\n",
        "    x = torch.sigmoid(x)\n",
        "    x = self.fc_2(x)\n",
        " \n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fo4exhg2SUE",
        "colab_type": "text"
      },
      "source": [
        "### Loss-function\n",
        "\n",
        "MSE is used as loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjB51OjZqJ-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.MSELoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuqGV4br2vfA",
        "colab_type": "text"
      },
      "source": [
        "### Optimizer\n",
        "To first get a result which is similar to the udacity project, I choose SGD as optimizer.\n",
        "\n",
        "```\n",
        "optimizer = optim.SGD(bikeSharingModel.parameters(), lr=hyperparameter['learning_rate'])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKSR0diz_zVO",
        "colab_type": "text"
      },
      "source": [
        "## Helper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oERur5zlPGeQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check gpu\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if train_on_gpu:\n",
        "  print(\"Train on GPU\")\n",
        "else:\n",
        "  print(\"Train on CPU\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmcszdFmXhc5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot(train_losses, validation_losses):\n",
        "  plt.plot(train_losses, label='Training loss')\n",
        "  plt.plot(validation_losses, label='Validation loss')\n",
        "  plt.legend()\n",
        "  _ = plt.ylim(0, 0.75)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg8BvTPN21Zn",
        "colab_type": "text"
      },
      "source": [
        "## Train, validate and test/inference\n",
        "Training is done with a batch size of random training data. Batch size is configured within `hyperparameter['batch_size']`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIw-zHyYBKOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "from ray import tune\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train_validate_with_tune_tracking(train_set, valid_set, hyperparameter, with_tune_tracking=False):\n",
        "\n",
        "  train_loader = DataLoader(train_set, shuffle=True, batch_size=hyperparameter['batch_size'])\n",
        "  valid_loader = DataLoader(valid_set, shuffle=True, batch_size=hyperparameter['batch_size'])\n",
        "  input_nodes = iter(train_loader).next()[0].shape[1]\n",
        "  bikeSharingModel = BikeSharingModel(input_nodes, hyperparameter['hidden_nodes'])\n",
        "  optimizer = optim.SGD(bikeSharingModel.parameters(), lr=hyperparameter['learning_rate'])\n",
        "  \n",
        "  if train_on_gpu:\n",
        "    bikeSharingModel.cuda()\n",
        "\n",
        "  train_losses, validation_losses = [], []\n",
        "  for epoch in range(1, hyperparameter['epochs']+1):\n",
        "    \n",
        "    features_batch, targets_batch = iter(train_loader).next() # train with one batch only to get similar results to implementation without pytorch\n",
        "\n",
        "    if train_on_gpu:\n",
        "      features_batch, targets_batch = features_batch.cuda(), targets_batch.cuda()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output = bikeSharingModel(features_batch)\n",
        "\n",
        "    loss = criterion(output, targets_batch)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    train_losses.append(loss.item())\n",
        "\n",
        "    # validate\n",
        "    loss_validation = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      bikeSharingModel.eval()\n",
        "      for features_valid, targets_valid in valid_loader:\n",
        "\n",
        "        if train_on_gpu:\n",
        "          features_valid, targets_valid = features_valid.cuda(), targets_valid.cuda()\n",
        "\n",
        "        output_validation = bikeSharingModel(features_valid)\n",
        "        loss_validation += criterion(output_validation, targets_valid).item()\n",
        "\n",
        "    bikeSharingModel.train()\n",
        "\n",
        "    loss_validation = loss_validation / (len(valid_loader.dataset) / hyperparameter['batch_size'])\n",
        "    validation_losses.append(loss_validation)\n",
        "\n",
        "    # log results\n",
        "    if with_tune_tracking:\n",
        "      tune.track.log(validation_loss_metric=loss_validation)\n",
        "    else:\n",
        "      sys.stdout.write(\"\\rProgress: {:2.1f}% Training loss: {:2.3f} Validation loss: {:2.3f} \".format(100*epoch/(hyperparameter['epochs']+1), loss, loss_validation))\n",
        "      sys.stdout.flush()\n",
        "\n",
        "  if not with_tune_tracking:\n",
        "    plot(train_losses, validation_losses)\n",
        "\n",
        "  return bikeSharingModel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuRF9VcCm7oo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model_to_test, features, targets):\n",
        "\n",
        "  with torch.no_grad():\n",
        "    model_to_test.eval()\n",
        "    if train_on_gpu:\n",
        "      features, targets = features.cuda(), targets.cuda()\n",
        "      \n",
        "    output = model_to_test(features)\n",
        "    test_loss = criterion(output, targets)\n",
        "    model_to_test.train()\n",
        "\n",
        "    print(\"\\nTest loss \", test_loss.item())\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8,4))\n",
        "    mean, std = feature_scaling_store['cnt']\n",
        "    ax.plot(output.cpu().numpy()*std + mean, label='Prediction')\n",
        "    ax.plot((targets_test['cnt']*std + mean).values, label='Data')\n",
        "    ax.set_xlim(right=len(output))\n",
        "    ax.legend()\n",
        "    dates = pd.to_datetime(rides_origin.iloc[test_data.index]['dteday'])\n",
        "    dates = dates.apply(lambda d: d.strftime('%b %d'))\n",
        "    ax.set_xticks(np.arange(len(dates))[12::24])\n",
        "    _ = ax.set_xticklabels(dates[12::24], rotation=45)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0JgM0Oq3gX8",
        "colab_type": "text"
      },
      "source": [
        "## Manual hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFe-RycNCorU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hyperparameter = {'learning_rate': 0.5, 'hidden_nodes': 25, 'epochs': 4000, 'batch_size': 128}\n",
        "\n",
        "trained_model = train_validate_with_tune_tracking(train_set, valid_set, hyperparameter)\n",
        "test(trained_model, torch.tensor(features_test.values).float(), torch.tensor(targets_test.values).float())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCovkuS8Q7Ug",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparamter tuning with tune\n",
        "\n",
        "### Search algorithm\n",
        "Choosing hyperopt\n",
        "\n",
        "Tune's HyperOptSearch uses hyperopt with TPE (Tree structured parzen estimators) see:\n",
        "\n",
        "https://ray.readthedocs.io/en/latest/tune-searchalg.html#hyperopt-search-tree-structured-parzen-estimators\n",
        "\n",
        "### Scheduler\n",
        "Choosing PBT, see:\n",
        "\n",
        "https://ray.readthedocs.io/en/latest/tune-schedulers.html#population-based-training-pbt\n",
        "\n",
        "### some notes on configuration\n",
        "\n",
        "First, to run multiple trials in parallel, the `resources_per_trial` param must be set accordingly. In my previous version I had set absolute values for CPU and GPU usage/trail. This is possible, but with 1 GPU in total, no parallel execution is possible. Therefore I switched adjust trial ressources with a percentage value, e.g. `resources_per_trial={\"cpu\": 0.2, \"gpu\": 0.2}`, to allow each trial 20% usage of the available resources.\n",
        "\n",
        "Second, with parallel execution and PBT, it is not possible to have model architecture specific values like `hidden_nodes` in the hyperparameters. As PBT changes the values of the hyperparameters as the model is instantiated, it is not possible to e.g. load weights for 10 hidden nodes in a changed model with 20 hidden nodes.\n",
        "\n",
        "To solve this and keep `hidden_nodes` as a hyperparameter, either choose a different scheduler or use tune's run_experiments-API, in which multiple experiments with different model archtitectures can be run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DF08qStlWP0A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
        "from hyperopt import hp, tpe\n",
        "from ray.tune.schedulers import PopulationBasedTraining\n",
        "\n",
        "def tune_it(config):\n",
        "  train_validate_with_tune_tracking(train_set, valid_set, config, with_tune_tracking=True)\n",
        "\n",
        "search_space = {'learning_rate': hp.choice('learning_rate', [0.00001, 0.0001, 0.001, 0.01, 0.1, 0.5, 0.7]),\n",
        "                'hidden_nodes': hp.choice('hidden_nodes', [10]),\n",
        "                'batch_size': hp.choice('batch_size', [128, 256, 10, 1])\n",
        "}\n",
        "\n",
        "hyperopt = HyperOptSearch(search_space,\n",
        "                          metric=\"validation_loss_metric\",\n",
        "                          mode=\"min\")\n",
        "\n",
        "pbt = PopulationBasedTraining(metric='validation_loss_metric',\n",
        "                              mode='min',\n",
        "                              hyperparam_mutations= search_space)\n",
        "\n",
        "tune_result = tune.run(tune_it,\n",
        "                   config={'epochs': 4000},\n",
        "                   local_dir='tune_logs',\n",
        "                   verbose=0,\n",
        "                   resources_per_trial={\"cpu\": 0.2, \"gpu\": 0.2},\n",
        "                   search_alg=hyperopt,\n",
        "                   num_samples = 50,\n",
        "                   scheduler = pbt,\n",
        "                   reuse_actors = False,\n",
        "                   resume = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeWOTgTXj4zn",
        "colab_type": "text"
      },
      "source": [
        "## Tune result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tr2qnmJgYFge",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from ray.tune import Analysis\n",
        "\n",
        "def print_config(tune_report):\n",
        "  batch_size = tune_report['config/batch_size'].item()\n",
        "  hidden_nodes = tune_report['config/hidden_nodes'].item()\n",
        "  learning_rate = tune_report['config/learning_rate'].item()\n",
        "  validation_loss_metric = tune_report['validation_loss_metric'].item()\n",
        "  print(\"Best config: 'batch_size': {}, 'hidden_nodes': {}, 'learning_rate': {}, validation loss: {}\".format(batch_size, hidden_nodes, learning_rate, validation_loss_metric))\n",
        "\n",
        "# best config access by directory\n",
        "report_0 = Analysis(\"/content/tune_logs/tune_it/\").dataframe().sort_values('validation_loss_metric').head(1)\n",
        "print_config(report_0)\n",
        "\n",
        "# best config access by function\n",
        "print(\"Best config: {} out of {} trials\".format(tune_result.get_best_config(metric='validation_loss_metric'), len(trials)))\n",
        "\n",
        "# best config access by dataframe - variant #1\n",
        "report_1 = tune_result.dataframe().sort_values('validation_loss_metric').head(1)\n",
        "print_config(report_1)\n",
        "\n",
        "# best config access by dataframe - variant #2\n",
        "report_2= tune_result.dataframe(metric=\"validation_loss_metric\", mode=\"min\").sort_values('validation_loss_metric').head(1)\n",
        "print_config(report_2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K04-YyjXToq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir tune_logs/tune_it"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zngicL0337N-",
        "colab_type": "text"
      },
      "source": [
        "### Test tune result\n",
        "Actual tune results `batch_size: 256, hidden_nodes: 10, learning_rate: 0.7 validation loss: 0.13463438223072796\n",
        "` for minimal validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGpL_oe54eog",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hyperparameter = {'learning_rate': 0.7, 'hidden_nodes': 10, 'epochs': 4000, 'batch_size': 256}\n",
        "\n",
        "trained_model = train_validate_with_tune_tracking(train_set, valid_set, hyperparameter)\n",
        "test(trained_model, torch.tensor(features_test.values).float(), torch.tensor(targets_test.values).float())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9-dmXi33EE0",
        "colab_type": "text"
      },
      "source": [
        "## Further steps/TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7WTb-tW2HQ-",
        "colab_type": "text"
      },
      "source": [
        "### Features and targets\n",
        "- test with adding field *workingday*\n",
        "- test with making field *holiday* categorical\n",
        "- test with delete field *year*\n",
        "- test with adding field *atemp*\n",
        "- check standard deviation\n",
        "- drop header in data\n",
        "- try different bag sizes for test/validation/train sets\n",
        "\n",
        "### Model\n",
        "- add dropout\n",
        "- build fcn best practice by Andrej Karpathy\n",
        "\n",
        "### Hyperparameter tuning\n",
        "- write documentation of used search algo and scheduler\n",
        "- save best model\n",
        "- hand different optimizer and model over to tune (in tune docs: models should be handed over as state_dict: https://ray.readthedocs.io/en/latest/using-ray-with-pytorch.html)\n",
        "- log multiple metrices at once\n",
        "- better understand tune result\n",
        "- does my combination of hyperopt and pbt make sense?\n",
        "- resume on error\n",
        "- example error\n",
        "```\n",
        "Traceback (most recent call last):\n",
        "  File \"/usr/local/lib/python3.6/dist-packages/ray/tune/trial_runner.py\", line 521, in _process_trial\n",
        "    self, trial, flat_result)\n",
        "  File \"/usr/local/lib/python3.6/dist-packages/ray/tune/schedulers/pbt.py\", line 245, in on_trial_result\n",
        "    self._exploit(trial_runner.trial_executor, trial, trial_to_clone)\n",
        "  File \"/usr/local/lib/python3.6/dist-packages/ray/tune/schedulers/pbt.py\", line 310, in _exploit\n",
        "    trial_to_clone, new_config)\n",
        "  File \"/usr/local/lib/python3.6/dist-packages/ray/tune/schedulers/pbt.py\", line 279, in _log_config_on_step\n",
        "    f.write(json.dumps(policy) + \"\\n\")\n",
        "  File \"/usr/lib/python3.6/json/__init__.py\", line 231, in dumps\n",
        "    return _default_encoder.encode(obj)\n",
        "  File \"/usr/lib/python3.6/json/encoder.py\", line 199, in encode\n",
        "    chunks = self.iterencode(o, _one_shot=True)\n",
        "  File \"/usr/lib/python3.6/json/encoder.py\", line 257, in iterencode\n",
        "    return _iterencode(o, 0)\n",
        "  File \"/usr/lib/python3.6/json/encoder.py\", line 180, in default\n",
        "    o.__class__.__name__)\n",
        "TypeError: Object of type 'Apply' is not JSON serializable\n",
        "```"
      ]
    }
  ]
}